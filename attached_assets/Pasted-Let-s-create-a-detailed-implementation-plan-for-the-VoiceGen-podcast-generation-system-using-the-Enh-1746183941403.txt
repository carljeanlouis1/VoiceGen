Let's create a detailed implementation plan for the VoiceGen podcast generation system using the Enhanced Continuity Architecture. I'll focus on how to build this within the current architecture you mentioned, which includes:

- React/Node.js architecture for multimedia processing
- Multiple AI models: GPT-4o, Claude 3.7 Sonnet
- Perplexity Sonar Pro for web search
- Various voice options for TTS
- Existing modular podcast creation capability

I'll create a comprehensive implementation guide that covers:
1. System architecture and data flow
2. Key components and their interactions
3. API design and implementations
4. Prompt engineering
5. Content processing pipeline
6. State management
7. Implementation steps

This will be a detailed technical guide that a Replit agent could follow to implement the enhanced continuity architecture in your existing VoiceGen framework.

# VoiceGen Enhanced Continuity Architecture Implementation Guide

// VoiceGen Enhanced Continuity Architecture - Implementation Guide

/**
 * SYSTEM ARCHITECTURE OVERVIEW
 * 
 * This implementation extends the existing VoiceGen architecture to support
 * long-form podcast generation (30-60 minutes) using the Enhanced Continuity
 * Architecture pattern.
 * 
 * Core Components:
 * 1. PodcastProjectManager - Tracks state and orchestrates the generation flow
 * 2. ResearchService - Handles topic analysis and web research via Perplexity
 * 3. NarrativeService - Creates the podcast structure and narrative guide
 * 4. ContentGenerator - Generates content with overlapping context windows
 * 5. CompilationService - Assembles the final script with intelligent merging
 * 6. AudioService - Converts the script to speech in manageable chunks
 */

// =====================================================================
// DATA MODELS
// =====================================================================

/**
 * Main project data structure that tracks the podcast generation state
 */
interface PodcastProject {
  id: string;
  topic: string;
  targetDuration: number; // minutes
  voice: string; // TTS voice option
  status: PodcastStatus;
  createdAt: Date;
  updatedAt: Date;
  completedAt?: Date;
  
  // Process data
  researchData?: ResearchData;
  narrativeGuide?: string;
  podcastStructure?: PodcastStructure;
  contentChunks?: ContentChunk[];
  finalScript?: string;
  audioUrl?: string;
  
  // Metrics/tracking
  progress: number; // 0-100%
  estimatedTokensUsed?: number;
  estimatedCost?: number;
}

type PodcastStatus = 
  | 'initializing'
  | 'researching' 
  | 'planning'
  | 'generating'
  | 'compiling'
  | 'converting'
  | 'complete'
  | 'failed';

interface ResearchData {
  topicAnalysis: TopicAnalysis;
  mainResearch: ResearchResult[];
  segmentResearch: Map<string, ResearchResult[]>;
}

interface TopicAnalysis {
  mainAreas: TopicArea[];
  targetAudience: string;
  keyQuestions: string[];
  suggestedApproach: string;
}

interface TopicArea {
  title: string;
  description: string;
  researchQuestions: string[];
  relevance: number; // 1-10 importance score
}

interface ResearchResult {
  query: string;
  results: PerplexityResult[];
  summary: string;
}

interface PodcastStructure {
  title: string;
  introduction: ContentSection;
  mainSegments: ContentSegment[];
  conclusion: ContentSection;
  estimatedDuration: number;
  estimatedTokens: number;
}

interface ContentSegment {
  id: string;
  title: string;
  sections: ContentSection[];
  estimatedDuration: number;
  estimatedTokens: number;
}

interface ContentSection {
  id: string;
  title: string;
  keyPoints: string[];
  talkingPoints: string[];
  estimatedDuration: number;
  estimatedTokens: number;
}

interface ContentChunk {
  id: string;
  position: number;
  sectionIds: string[];
  content: string;
  overlapStart?: string; // Overlap with previous chunk
  overlapEnd?: string; // Overlap with next chunk
  modelUsed: 'gpt-4o' | 'claude-3-7-sonnet';
  generatedAt: Date;
  contextUsed: string; // Description of context used for this generation
}

// =====================================================================
// SERVICE IMPLEMENTATIONS
// =====================================================================

/**
 * Main orchestrator class that coordinates the podcast generation process
 */
class PodcastProjectManager {
  private researchService: ResearchService;
  private narrativeService: NarrativeService;
  private contentGenerator: ContentGenerator;
  private compilationService: CompilationService;
  private audioService: AudioService;
  
  constructor() {
    this.researchService = new ResearchService();
    this.narrativeService = new NarrativeService();
    this.contentGenerator = new ContentGenerator();
    this.compilationService = new CompilationService();
    this.audioService = new AudioService();
  }
  
  /**
   * Creates a new podcast project and initiates the generation process
   */
  async createPodcast(params: {
    topic: string;
    targetDuration: number;
    voice: string;
  }): Promise<string> {
    const projectId = generateUUID();
    
    // Initialize project
    const project: PodcastProject = {
      id: projectId,
      topic: params.topic,
      targetDuration: params.targetDuration,
      voice: params.voice,
      status: 'initializing',
      createdAt: new Date(),
      updatedAt: new Date(),
      progress: 0
    };
    
    // Save initial project
    await this.saveProject(project);
    
    // Start async generation process
    this.generatePodcast(projectId).catch(error => {
      console.error(`Error generating podcast ${projectId}:`, error);
      this.updateProjectStatus(projectId, 'failed');
    });
    
    return projectId;
  }
  
  /**
   * Main podcast generation pipeline
   */
  private async generatePodcast(projectId: string): Promise<void> {
    try {
      // 1. Research Phase
      await this.performResearch(projectId);
      
      // 2. Planning Phase
      await this.createPodcastPlan(projectId);
      
      // 3. Content Generation Phase
      await this.generateContent(projectId);
      
      // 4. Compilation Phase
      await this.compileFinalScript(projectId);
      
      // 5. Audio Conversion Phase
      await this.generateAudio(projectId);
      
      // Update project as complete
      await this.updateProjectStatus(projectId, 'complete', 100);
      
    } catch (error) {
      console.error(`Error in podcast generation pipeline for ${projectId}:`, error);
      await this.updateProjectStatus(projectId, 'failed');
      throw error;
    }
  }
  
  /**
   * Step 1: Research Phase - Topic analysis and web research
   */
  private async performResearch(projectId: string): Promise<void> {
    await this.updateProjectStatus(projectId, 'researching', 5);
    
    // Get project data
    const project = await this.getProject(projectId);
    
    // 1.1 Analyze topic using Claude to break it down
    const topicAnalysis = await this.researchService.analyzeTopicWithClaude(project.topic);
    await this.updateProject(projectId, {
      researchData: {
        topicAnalysis,
        mainResearch: [],
        segmentResearch: new Map()
      },
      progress: 10
    });
    
    // 1.2 Perform initial research using Perplexity
    const researchQueries = this.extractResearchQueries(topicAnalysis);
    const mainResearch = await this.researchService.performBatchResearch(researchQueries);
    await this.updateProject(projectId, {
      researchData: {
        ...project.researchData,
        mainResearch
      },
      progress: 20
    });
  }
  
  /**
   * Step 2: Planning Phase - Create podcast structure and narrative guide
   */
  private async createPodcastPlan(projectId: string): Promise<void> {
    await this.updateProjectStatus(projectId, 'planning', 25);
    
    // Get project data
    const project = await this.getProject(projectId);
    
    // 2.1 Create podcast structure using Claude
    const podcastStructure = await this.narrativeService.createPodcastStructure(
      project.topic,
      project.researchData.topicAnalysis,
      project.researchData.mainResearch,
      project.targetDuration
    );
    
    await this.updateProject(projectId, { 
      podcastStructure,
      progress: 30
    });
    
    // 2.2 Create narrative guide using Claude
    const narrativeGuide = await this.narrativeService.createNarrativeGuide(
      project.topic,
      podcastStructure
    );
    
    await this.updateProject(projectId, { 
      narrativeGuide,
      progress: 35 
    });
    
    // 2.3 Perform segment-specific research for deeper content
    const segmentResearch = new Map<string, ResearchResult[]>();
    
    for (const segment of podcastStructure.mainSegments) {
      const segmentQueries = [
        `${project.topic} ${segment.title}`,
        ...segment.sections.map(s => `${project.topic} ${s.title}`)
      ];
      
      const research = await this.researchService.performBatchResearch(segmentQueries);
      segmentResearch.set(segment.id, research);
      
      // Incremental progress update
      const progressIncrement = 15 / podcastStructure.mainSegments.length;
      project.progress += progressIncrement;
      await this.updateProject(projectId, { 
        progress: Math.min(50, project.progress),
        researchData: {
          ...project.researchData,
          segmentResearch
        }
      });
    }
  }
  
  /**
   * Step 3: Content Generation Phase - Generate content with overlapping windows
   */
  private async generateContent(projectId: string): Promise<void> {
    await this.updateProjectStatus(projectId, 'generating', 55);
    
    // Get project data
    const project = await this.getProject(projectId);
    
    // 3.1 Calculate chunk strategy based on token estimates
    const chunkingStrategy = this.planContentChunks(
      project.podcastStructure, 
      project.targetDuration
    );
    
    // 3.2 Generate content chunks with overlapping windows
    const contentChunks: ContentChunk[] = [];
    let currentContext = ""; // Initial context is empty
    
    for (const [index, chunkPlan] of chunkingStrategy.entries()) {
      // Generate content for this chunk
      const chunk = await this.contentGenerator.generateContentChunk({
        project,
        chunkPlan,
        previousContext: currentContext,
        isFirstChunk: index === 0,
        isLastChunk: index === chunkingStrategy.length - 1,
        relativePosition: index / chunkingStrategy.length
      });
      
      contentChunks.push(chunk);
      
      // Update context for next chunk (use end of current chunk)
      const OVERLAP_SIZE = 1000; // characters to keep as context
      currentContext = chunk.content.slice(-OVERLAP_SIZE);
      
      // Store overlap information
      if (index < chunkingStrategy.length - 1) {
        chunk.overlapEnd = currentContext;
      }
      
      // Update progress incrementally
      const progressIncrement = 30 / chunkingStrategy.length;
      project.progress += progressIncrement;
      await this.updateProject(projectId, {
        contentChunks,
        progress: Math.min(85, project.progress)
      });
    }
  }
  
  /**
   * Step 4: Compilation Phase - Assemble the final script
   */
  private async compileFinalScript(projectId: string): Promise<void> {
    await this.updateProjectStatus(projectId, 'compiling', 90);
    
    // Get project data
    const project = await this.getProject(projectId);
    
    // Compile the final script from content chunks
    const finalScript = await this.compilationService.compileFinalScript(
      project.contentChunks,
      project.podcastStructure
    );
    
    // Update project with final script
    await this.updateProject(projectId, {
      finalScript,
      progress: 95
    });
  }
  
  /**
   * Step 5: Audio Conversion Phase - Generate audio
   */
  private async generateAudio(projectId: string): Promise<void> {
    await this.updateProjectStatus(projectId, 'converting', 96);
    
    // Get project data
    const project = await this.getProject(projectId);
    
    // Generate audio from the script
    const audioUrl = await this.audioService.generateLongformAudio(
      project.finalScript,
      project.voice
    );
    
    // Update project with audio URL
    await this.updateProject(projectId, {
      audioUrl,
      progress: 100
    });
  }
  
  /**
   * Plans how to divide content into chunks for generation
   */
  private planContentChunks(
    structure: PodcastStructure,
    targetDuration: number
  ): ChunkPlan[] {
    const TOKENS_PER_MINUTE = 750; // Approximate tokens for 1 minute of audio
    const MAX_CHUNK_SIZE = 3000; // Target tokens per generation (excluding overlap)
    
    // Flatten all sections
    const allSections = [
      structure.introduction,
      ...structure.mainSegments.flatMap(segment => segment.sections),
      structure.conclusion
    ];
    
    const chunkPlans: ChunkPlan[] = [];
    let currentChunk: ChunkPlan = {
      sections: [],
      totalTokens: 0,
      position: 0
    };
    
    // Group sections into chunks respecting token limits
    for (const section of allSections) {
      const sectionTokens = section.estimatedTokens || 
                           (section.estimatedDuration * TOKENS_PER_MINUTE);
      
      // If adding this section would exceed our chunk size and we already have content,
      // save current chunk and start a new one
      if (currentChunk.totalTokens > 0 && 
          currentChunk.totalTokens + sectionTokens > MAX_CHUNK_SIZE) {
        chunkPlans.push(currentChunk);
        
        currentChunk = {
          sections: [section],
          totalTokens: sectionTokens,
          position: chunkPlans.length
        };
      } else {
        // Add section to current chunk
        currentChunk.sections.push(section);
        currentChunk.totalTokens += sectionTokens;
      }
    }
    
    // Add the final chunk if it has any sections
    if (currentChunk.sections.length > 0) {
      chunkPlans.push(currentChunk);
    }
    
    return chunkPlans;
  }
  
  // Helper methods for project management
  private async saveProject(project: PodcastProject): Promise<void> {
    // Implementation depends on your storage mechanism
  }
  
  private async getProject(projectId: string): Promise<PodcastProject> {
    // Implementation depends on your storage mechanism
    return {} as PodcastProject; // Placeholder
  }
  
  private async updateProject(
    projectId: string, 
    updates: Partial<PodcastProject>
  ): Promise<void> {
    // Implementation depends on your storage mechanism
  }
  
  private async updateProjectStatus(
    projectId: string, 
    status: PodcastStatus, 
    progress?: number
  ): Promise<void> {
    await this.updateProject(projectId, { 
      status, 
      progress: progress !== undefined ? progress : undefined,
      updatedAt: new Date()
    });
  }
  
  private extractResearchQueries(topicAnalysis: TopicAnalysis): string[] {
    return topicAnalysis.mainAreas.flatMap(area => area.researchQuestions);
  }
}

/**
 * Interface for chunk planning
 */
interface ChunkPlan {
  sections: ContentSection[];
  totalTokens: number;
  position: number;
}

/**
 * Handles research using Perplexity and topic analysis with Claude
 */
class ResearchService {
  private perplexityClient: any; // Your Perplexity API client
  private claudeClient: any; // Your Claude API client
  
  constructor() {
    // Initialize clients
  }
  
  /**
   * Analyzes the podcast topic using Claude to break it down into areas
   */
  async analyzeTopicWithClaude(topic: string): Promise<TopicAnalysis> {
    const prompt = `
You are an expert podcast researcher. Your task is to analyze the following podcast topic: "${topic}".

Create a comprehensive research plan by:
1. Breaking down the topic into 4-5 main areas to cover
2. For each area, identify 3-5 specific research questions
3. Identify the target audience for this topic
4. Suggest potential angles, approaches, or controversies to explore
5. Rate each area's relevance on a scale of 1-10

Format your response as a structured JSON object with the following schema:
{
  "mainAreas": [
    {
      "title": "string",
      "description": "string",
      "researchQuestions": ["string"],
      "relevance": number
    }
  ],
  "targetAudience": "string",
  "keyQuestions": ["string"],
  "suggestedApproach": "string"
}
`;

    const response = await this.claudeClient.messages.create({
      model: 'claude-3-7-sonnet-20250219',
      max_tokens: 4000,
      system: "You are an expert podcast topic researcher who analyzes topics and creates structured research plans. Always return valid JSON.",
      messages: [{ role: 'user', content: prompt }]
    });
    
    try {
      return JSON.parse(response.content[0].text);
    } catch (error) {
      console.error("Failed to parse Claude's topic analysis response as JSON:", error);
      throw new Error("Invalid response format from topic analysis");
    }
  }
  
  /**
   * Performs a batch of research queries using Perplexity
   */
  async performBatchResearch(queries: string[]): Promise<ResearchResult[]> {
    const results: ResearchResult[] = [];
    
    for (const query of queries) {
      try {
        // Call Perplexity API
        const searchResults = await this.perplexityClient.search(query);
        
        // Create summary of results using the data itself
        const summary = this.createSearchSummary(searchResults);
        
        results.push({
          query,
          results: searchResults,
          summary
        });
      } catch (error) {
        console.error(`Error performing research for query "${query}":`, error);
        // Add empty result to maintain query order
        results.push({
          query,
          results: [],
          summary: "Research failed for this query."
        });
      }
    }
    
    return results;
  }
  
  /**
   * Creates a short summary of search results
   */
  private createSearchSummary(results: any[]): string {
    // Basic implementation - in a full system, might use Claude for this
    if (!results || results.length === 0) {
      return "No results found.";
    }
    
    const topResults = results.slice(0, 3);
    return topResults.map(r => r.title || "Untitled").join("; ");
  }
}

/**
 * Handles podcast structure and narrative planning using Claude
 */
class NarrativeService {
  private claudeClient: any; // Your Claude API client
  
  constructor() {
    // Initialize client
  }
  
  /**
   * Creates the podcast structure with Claude
   */
  async createPodcastStructure(
    topic: string,
    topicAnalysis: TopicAnalysis,
    mainResearch: ResearchResult[],
    targetDuration: number
  ): Promise<PodcastStructure> {
    // Prepare research summaries to include in prompt
    const researchSummary = mainResearch.map(r => 
      `Query: ${r.query}\nSummary: ${r.summary}`
    ).join("\n\n");
    
    const prompt = `
You are an expert podcast producer. Your task is to create a detailed structure for a ${targetDuration}-minute podcast on the topic: "${topic}".

Here is the topic analysis:
${JSON.stringify(topicAnalysis, null, 2)}

Here is a summary of research on this topic:
${researchSummary}

Create a podcast structure with:
1. An engaging introduction (1-2 minutes)
2. 4-6 main segments that build a compelling narrative
3. A conclusion that ties everything together (1-2 minutes)

For each segment and section:
1. Provide a clear title
2. List key points to cover
3. Estimate the duration (in minutes)
4. Estimate tokens needed (approx. 750 tokens per minute)

Format your response as a structured JSON object following this schema:
{
  "title": "string",
  "introduction": {
    "id": "string",
    "title": "string",
    "keyPoints": ["string"],
    "talkingPoints": ["string"],
    "estimatedDuration": number,
    "estimatedTokens": number
  },
  "mainSegments": [
    {
      "id": "string",
      "title": "string",
      "sections": [
        {
          "id": "string",
          "title": "string",
          "keyPoints": ["string"],
          "talkingPoints": ["string"],
          "estimatedDuration": number,
          "estimatedTokens": number
        }
      ],
      "estimatedDuration": number,
      "estimatedTokens": number
    }
  ],
  "conclusion": {
    "id": "string",
    "title": "string",
    "keyPoints": ["string"],
    "talkingPoints": ["string"],
    "estimatedDuration": number,
    "estimatedTokens": number
  },
  "estimatedDuration": number,
  "estimatedTokens": number
}

Ensure the total duration adds up to approximately ${targetDuration} minutes.
`;

    const response = await this.claudeClient.messages.create({
      model: 'claude-3-7-sonnet-20250219',
      max_tokens: 4000,
      system: "You are an expert podcast producer who creates detailed, well-structured podcast outlines. Always return valid JSON.",
      messages: [{ role: 'user', content: prompt }]
    });
    
    try {
      return JSON.parse(response.content[0].text);
    } catch (error) {
      console.error("Failed to parse Claude's podcast structure response as JSON:", error);
      throw new Error("Invalid response format from podcast structure generation");
    }
  }
  
  /**
   * Creates a narrative guide for the podcast using Claude
   */
  async createNarrativeGuide(
    topic: string,
    structure: PodcastStructure
  ): Promise<string> {
    const prompt = `
You are an expert podcast narrative designer. Your task is to create a comprehensive narrative guide for a ${structure.estimatedDuration}-minute podcast titled "${structure.title}" on the topic: "${topic}".

The podcast has this structure:
${JSON.stringify(structure, null, 2)}

Create a narrative guide that:
1. Establishes the overall "story arc" of this podcast
2. Identifies 3-5 key themes that should recur throughout
3. Establishes the voice and tone (as "Arion Vale")
4. Notes how earlier segments should foreshadow later ones
5. Describes how later segments should callback to earlier ones
6. Suggests metaphors, analogies, or examples that could work across segments
7. Defines the audience journey from start to finish

This guide will be used to ensure all segments of the podcast maintain narrative continuity and flow naturally together, even when generated separately.
`;

    const response = await this.claudeClient.messages.create({
      model: 'claude-3-7-sonnet-20250219',
      max_tokens: 4000,
      system: "You are an expert podcast narrative designer who creates comprehensive guides for narrative flow and continuity.",
      messages: [{ role: 'user', content: prompt }]
    });
    
    return response.content[0].text;
  }
}

/**
 * Handles content generation with overlapping context windows
 */
class ContentGenerator {
  private gpt4oClient: any; // Your OpenAI API client
  
  constructor() {
    // Initialize client
  }
  
  /**
   * Generates a content chunk with overlapping context
   */
  async generateContentChunk(params: {
    project: PodcastProject;
    chunkPlan: ChunkPlan;
    previousContext: string;
    isFirstChunk: boolean;
    isLastChunk: boolean;
    relativePosition: number;
  }): Promise<ContentChunk> {
    const { 
      project, 
      chunkPlan, 
      previousContext, 
      isFirstChunk, 
      isLastChunk,
      relativePosition 
    } = params;
    
    // Extract relevant narrative guidance from the full guide
    const narrativeGuidance = this.extractNarrativeGuidance(
      project.narrativeGuide,
      relativePosition,
      isFirstChunk,
      isLastChunk
    );
    
    // Format sections for the prompt
    const sectionsFormatted = chunkPlan.sections.map(section => `
SECTION: ${section.title}
KEY POINTS:
${section.keyPoints.map(point => `- ${point}`).join('\n')}
${section.talkingPoints ? `TALKING POINTS:\n${section.talkingPoints.map(point => `- ${point}`).join('\n')}` : ''}
    `).join('\n\n');
    
    // Create the generation prompt
    const prompt = `
You are Arion Vale, an engaging podcast host. You're creating ${isFirstChunk ? "the introduction" : isLastChunk ? "the conclusion" : "a middle segment"} of a podcast on "${project.topic}".

${previousContext ? "CONTEXT FROM PREVIOUS CONTENT (continue naturally from this):\n" + previousContext : ""}

NARRATIVE GUIDANCE (to maintain flow and continuity):
${narrativeGuidance}

${isFirstChunk ? "Start with an engaging hook that draws listeners in and introduces the topic in a compelling way." : ""}
${isLastChunk ? "End with a satisfying conclusion that ties everything together and leaves listeners with something meaningful to reflect on." : ""}

In this segment, cover these sections:
${sectionsFormatted}

Write in a conversational, natural style as if you're speaking directly to listeners. Include:
1. Smooth transitions between points
2. References to earlier content when relevant (${isFirstChunk ? "foreshadow what's to come" : "refer back to earlier points"})
3. Natural speech patterns with contractions and occasional pauses (...)
4. Engaging analogies or examples to illustrate complex ideas
5. Occasional rhetorical questions to engage the listener

IMPORTANT: 
- Your content should flow naturally from any preceding content
- Don't repeat information already covered
- Maintain consistent terminology and tone throughout
- Write the script as spoken content, not as an essay

Write ONLY the podcast script content, not section headers or notes.
`;

    const response = await this.gpt4oClient.chat.completions.create({
      model: 'gpt-4o',
      temperature: 0.7,
      max_tokens: 4000,
      messages: [
        { role: 'system', content: 'You are Arion Vale, an engaging podcast host known for natural conversational flow and insightful content, speaking directly to listeners.' },
        { role: 'user', content: prompt }
      ]
    });
    
    const content = response.choices[0].message.content;
    
    // Create the chunk record
    return {
      id: `chunk-${chunkPlan.position}`,
      position: chunkPlan.position,
      sectionIds: chunkPlan.sections.map(s => s.id),
      content,
      overlapStart: previousContext || undefined,
      modelUsed: 'gpt-4o',
      generatedAt: new Date(),
      contextUsed: `Position: ${relativePosition.toFixed(2)}, First: ${isFirstChunk}, Last: ${isLastChunk}`
    };
  }
  
  /**
   * Extracts position-specific guidance from the narrative guide
   */
  private extractNarrativeGuidance(
    narrativeGuide: string,
    relativePosition: number,
    isFirstChunk: boolean,
    isLastChunk: boolean
  ): string {
    // In a full implementation, this would parse the narrative guide
    // and extract the most relevant sections based on position
    
    // For simplicity, here's a position-based approach
    if (isFirstChunk) {
      return `OPENING STAGE: Establish the main themes and questions. Use an inviting, curious tone that draws listeners in and hints at what's to come. Create intrigue and set expectations.`;
    } else if (isLastChunk) {
      return `CONCLUSION STAGE: Bring everything together. Reference key insights from throughout the podcast. Provide synthesis and meaningful takeaways. End with something thought-provoking.`;
    } else if (relativePosition < 0.33) {
      return `EARLY DEVELOPMENT: Build on the introduction. Start deepening the exploration of key concepts. Provide context and background that will be important later.`;
    } else if (relativePosition < 0.66) {
      return `MAIN EXPLORATION: This is the core of the podcast. Fully explore the key concepts with detailed examples and insights. Make connections between ideas introduced earlier.`;
    } else {
      return `SYNTHESIS STAGE: Begin connecting the ideas explored earlier. Highlight patterns and insights. Start preparing for the conclusion.`;
    }
  }
}

/**
 * Handles compilation of the final script
 */
class CompilationService {
  constructor() {
    // No initialization needed
  }
  
  /**
   * Compiles the final script from content chunks
   */
  async compileFinalScript(
    contentChunks: ContentChunk[],
    structure: PodcastStructure
  ): Promise<string> {
    // Sort chunks by position
    const orderedChunks = [...contentChunks].sort((a, b) => a.position - b.position);
    
    // 1. Direct assembly with intelligent overlap management
    const rawScript = this.assembleWithOverlapManagement(orderedChunks);
    
    // 2. Add structure elements (timestamps, chapter markers, etc.)
    const formattedScript = this.addFormattingElements(rawScript, structure);
    
    return formattedScript;
  }
  
  /**
   * Assembles chunks with intelligent overlap handling
   */
  private assembleWithOverlapManagement(chunks: ContentChunk[]): string {
    if (chunks.length === 0) {
      return "";
    }
    
    if (chunks.length === 1) {
      return chunks[0].content;
    }
    
    let assembledScript = chunks[0].content;
    
    // For each subsequent chunk, find intelligent joining point
    for (let i = 1; i < chunks.length; i++) {
      const prevChunk = chunks[i - 1];
      const currentChunk = chunks[i];
      
      // Get the overlap between chunks (if available)
      const overlapEnd = prevChunk.overlapEnd || "";
      const overlapStart = currentChunk.overlapStart || "";
      
      if (!overlapEnd || !overlapStart) {
        // No overlap information, simple concatenation
        assembledScript += "\n\n" + currentChunk.content;
        continue;
      }
      
      // Find where to join the chunks
      const joinPosition = this.findOptimalJoinPosition(
        assembledScript,
        currentChunk.content,
        overlapEnd
      );
      
      if (joinPosition > 0) {
        // Join at the optimal position
        assembledScript = assembledScript.substring(0, joinPosition) + 
                         currentChunk.content.substring(
                           this.findOverlapEndPosition(currentChunk.content, overlapStart)
                         );
      } else {
        // Fallback: add with a paragraph break
        assembledScript += "\n\n" + currentChunk.content;
      }
    }
    
    return assembledScript;
  }
  
  /**
   * Finds optimal position to join two chunks
   */
  private findOptimalJoinPosition(text1: string, text2: string, overlap: string): number {
    if (!overlap || overlap.length < 20) {
      return -1;
    }
    
    // Look for the overlap at the end of text1
    const overlapPosition = text1.lastIndexOf(overlap);
    if (overlapPosition >= 0) {
      return overlapPosition;
    }
    
    // If exact match not found, try sentence boundaries in the last 25% of text1
    const lastQuarter = text1.substring(Math.floor(text1.length * 0.75));
    const sentences = lastQuarter.split(/(?<=[.!?])\s+/);
    
    if (sentences.length <= 1) {
      return -1;
    }
    
    // Try to find a good sentence boundary
    const lastCompleteSentencePosition = text1.lastIndexOf(sentences[sentences.length - 2]);
    if (lastCompleteSentencePosition > 0) {
      return lastCompleteSentencePosition + sentences[sentences.length - 2].length;
    }
    
    return -1;
  }
  
  /**
   * Finds where overlap ends in the second chunk
   */
  private findOverlapEndPosition(text: string, overlap: string): number {
    if (!overlap || overlap.length < 20) {
      return 0;
    }
    
    const overlapPosition = text.indexOf(overlap);
    if (overlapPosition >= 0) {
      return overlapPosition + overlap.length;
    }
    
    return 0;
  }
  
  /**
   * Adds formatting elements to the script
   */
  private addFormattingElements(script: string, structure: PodcastStructure): string {
    let formattedScript = `# ${structure.title}\n\n`;
    
    // Add introduction header
    formattedScript += `## Introduction: ${structure.introduction.title}\n\n`;
    
    // Split the script into approximate segments based on estimated durations
    const totalDuration = structure.estimatedDuration;
    const scriptLength = script.length;
    const charsPerMinute = scriptLength / totalDuration;
    
    let currentPosition = 0;
    let currentTime = 0;
    
    // Add introduction (estimated)
    const introLength = Math.floor(structure.introduction.estimatedDuration * charsPerMinute);
    formattedScript += script.substring(0, introLength) + "\n\n";
    currentPosition += introLength;
    currentTime += structure.introduction.estimatedDuration;
    
    // Add main segments with timestamps
    for (const segment of structure.mainSegments) {
      // Add segment header with timestamp
      const timestamp = this.formatTimestamp(currentTime);
      formattedScript += `## [${timestamp}] ${segment.title}\n\n`;
      
      // Add segment content (estimated)
      const segmentLength = Math.floor(segment.estimatedDuration * charsPerMinute);
      const segmentEndPosition = Math.min(currentPosition + segmentLength, scriptLength);
      
      formattedScript += script.substring(currentPosition, segmentEndPosition) + "\n\n";
      
      currentPosition = segmentEndPosition;
      currentTime += segment.estimatedDuration;
    }
    
    // Add conclusion with timestamp
    const conclusionTimestamp = this.formatTimestamp(currentTime);
    formattedScript += `## [${conclusionTimestamp}] Conclusion: ${structure.conclusion.title}\n\n`;
    
    // Add remaining script as conclusion
    formattedScript += script.substring(currentPosition);
    
    return formattedScript;
  }
  
  /**
   * Formats a timestamp
   */
  private formatTimestamp(minutes: number): string {
    const hrs = Math.floor(minutes / 60);
    const mins = Math.floor(minutes % 60);
    return `${hrs.toString().padStart(2, '0')}:${mins.toString().padStart(2, '0')}`;
  }
}

/**
 * Handles audio generation for long-form content
 */
class AudioService {
  private openaiClient: any; // Your OpenAI API client
  
  constructor() {
    // Initialize client
  }
  
  /**
   * Generates audio for a long-form script by chunking
   */
  async generateLongformAudio(script: string, voice: string): Promise<string> {
    // Split script into manageable chunks for TTS
    const scriptChunks = this.splitScriptForTTS(script);
    
    // Generate audio for each chunk
    const audioChunkPaths = await Promise.all(
      scriptChunks.map((chunk, index) => 
        this.generateAudioChunk(chunk, voice, index)
      )
    );
    
    // Concatenate audio files
    const finalAudioPath = await this.concatenateAudioFiles(audioChunkPaths);
    
    return finalAudioPath;
  }
  
  /**
   * Splits the script into TTS-friendly chunks
   */
  private splitScriptForTTS(script: string): string[] {
    const MAX_CHARS = 4000; // Conservative limit for TTS
    const chunks: string[] = [];
    
    // Split on paragraph boundaries
    const paragraphs = script.split('\n\n');
    let currentChunk = '';
    
    for (const paragraph of paragraphs) {
      // Skip markdown headers
      if (paragraph.startsWith('#')) {
        continue;
      }
      
      // If adding this paragraph would exceed limit, start a new chunk
      if (currentChunk.length + paragraph.length + 2 > MAX_CHARS && currentChunk.length > 0) {
        chunks.push(currentChunk);
        currentChunk = paragraph;
      } else {
        // Add paragraph to current chunk
        currentChunk = currentChunk.length === 0 
          ? paragraph 
          : `${currentChunk}\n\n${paragraph}`;
      }
    }
    
    // Add the last chunk if not empty
    if (currentChunk.length > 0) {
      chunks.push(currentChunk);
    }
    
    return chunks;
  }
  
  /**
   * Generates audio for a single chunk
   */
  private async generateAudioChunk(
    chunk: string, 
    voice: string, 
    index: number
  ): Promise<string> {
    try {
      const response = await this.openaiClient.audio.speech.create({
        model: 'tts-1',
        voice,
        input: chunk,
      });
      
      // Save audio to file
      const chunkPath = `temp/audio_chunk_${index}.mp3`;
      await this.saveAudioBuffer(response, chunkPath);
      
      return chunkPath;
    } catch (error) {
      console.error(`Error generating audio for chunk ${index}:`, error);
      throw error;
    }
  }
  
  /**
   * Saves audio buffer to file
   */
  private async saveAudioBuffer(audioResponse: any, path: string): Promise<void> {
    // Implementation depends on your file system approach
  }
  
  /**
   * Concatenates audio files into a single file
   */
  private async concatenateAudioFiles(filePaths: string[]): Promise<string> {
    // In a real implementation, use ffmpeg or similar to concatenate
    // For this example, we'll return a placeholder
    return 'path/to/final/audio.mp3';
  }
}

/**
 * Utility function to generate UUIDs
 */
function generateUUID(): string {
  return 'xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx'.replace(/[xy]/g, function(c) {
    const r = Math.random() * 16 | 0;
    const v = c === 'x' ? r : (r & 0x3 | 0x8);
    return v.toString(16);
  });
}

## Step-by-Step Implementation Instructions for Replit Agent

To implement the Enhanced Continuity Architecture in your VoiceGen app, follow these instructions:

### 1. Set Up Core Data Models

Create the data models defined in the implementation guide. Start with:

```typescript
// models/PodcastProject.ts
export interface PodcastProject {
  id: string;
  topic: string;
  targetDuration: number; // minutes
  voice: string; // TTS voice option
  status: PodcastStatus;
  // ... other fields from implementation guide
}

// Add other interfaces: ResearchData, TopicAnalysis, etc.
```

### 2. Implement Service Classes

Implement each service class in separate files:

```typescript
// services/ResearchService.ts
import { TopicAnalysis, ResearchResult } from '../models/PodcastModels';
import { PerplexityClient } from '../clients/PerplexityClient';
import { AnthropicClient } from '../clients/AnthropicClient';

export class ResearchService {
  // Implement methods from guide
}

// Repeat for other services: NarrativeService, ContentGenerator, etc.
```

### 3. Create API Client Wrappers

```typescript
// clients/AnthropicClient.ts
import { AnthropicAI } from '@anthropic-ai/sdk';

export class AnthropicClient {
  private client: AnthropicAI;
  
  constructor(apiKey: string) {
    this.client = new AnthropicAI({ apiKey });
  }
  
  // Wrap API methods
}

// Repeat for OpenAIClient and PerplexityClient
```

### 4. Set Up Backend API Routes

```typescript
// server/routes/podcast.ts
import express from 'express';
import { PodcastProjectManager } from '../services/PodcastProjectManager';

const router = express.Router();
const projectManager = new PodcastProjectManager();

// Create podcast
router.post('/podcasts', async (req, res) => {
  try {
    const { topic, targetDuration, voice } = req.body;
    const projectId = await projectManager.createPodcast({
      topic, targetDuration, voice
    });
    res.json({ id: projectId });
  } catch (error) {
    res.status(500).json({ error: error.message });
  }
});

// Get podcast status
router.get('/podcasts/:id', async (req, res) => {
  try {
    const project = await projectManager.getProject(req.params.id);
    res.json(project);
  } catch (error) {
    res.status(404).json({ error: 'Podcast not found' });
  }
});

export default router;
```

### 5. Implement Storage Manager

```typescript
// services/StorageManager.ts
export class StorageManager {
  // Methods for saving and retrieving podcast projects
}
```

### 6. Create Frontend Components

```typescript
// client/components/PodcastCreator.tsx
import React, { useState } from 'react';
import axios from 'axios';

export const PodcastCreator = () => {
  // React component for podcast creation UI
}

// Similar components for podcast progress, player, etc.
```

### 7. Implementation Priority Order

Here's the order to implement everything:

1. Data models
2. Basic storage (in-memory first, then persistent)
3. API clients
4. Core services (start with ResearchService)
5. Backend API routes
6. Main orchestrator (PodcastProjectManager)
7. Frontend components
8. Audio processing

## Key Implementation Notes

1. **Prompt Engineering**: The implementation includes carefully crafted prompts for each AI model. These are critical - test and refine them for best results.

2. **Overlapping Generation**: The core innovation is in ContentGenerator, where each chunk gets previous content as context and continues it naturally.

3. **Efficient Storage**: For large podcast projects, implement partial loading to avoid memory issues.

4. **Progressive Updates**: Update the frontend with progress regularly. The architecture is designed for background processing.

5. **Error Handling**: Implement comprehensive error handling and retries, especially for AI API calls.

6. **Chunk Management**: The intelligent overlap detection in CompilationService is key to seamless narrative flow.

7. **Modular Approach**: Each component is self-contained so you can implement incrementally.

## Testing the Implementation

1. Start by testing each service independently with small examples
2. Test the research flow with simple topics
3. Test content generation with predefined structures
4. Test compilation with manually created chunks
5. Finally, test the end-to-end flow

Once basic functionality works, focus on refining the prompts and overlap handling for better narrative quality.

## Advanced Features to Add Later

1. User feedback during generation
2. Ability to regenerate specific segments
3. Voice customization options
4. Topic suggestion based on trending searches
5. Multiple narrator voices for interview-style podcasts

This implementation provides a foundation that can be extended with these features once the core functionality is working.

Would you like me to elaborate on any specific part of the implementation?